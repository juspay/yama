# Cline Rules for Yama
# Project-specific patterns and intelligence

## Code Review Comment Posting

### MCP Integration Requirements
- When posting inline comments via Bitbucket MCP:
  - Code snippets MUST include the exact diff prefix (+, -, or space)
  - Always validate that snippets exist in the actual diff before posting
  - Use 'best' match strategy instead of 'strict' for flexibility
  - Include search_context with before/after lines when available
  - Line numbers from diff hunks need careful calculation

### Line Number Extraction from Diffs
- Hunk headers format: @@ -oldStart,oldCount +newStart,newCount @@
- The numbers in hunk headers are 1-based starting line numbers
- Must track current position carefully as we iterate
- Added lines (+) only increment new line counter
- Removed lines (-) only increment old line counter
- Context lines (space) increment both counters
- Line number calculation is complex and may have edge cases

### Path Handling
- Files can have complex nested paths
- Need to generate all possible path variations for matching
- Common variations include:
  - With/without a/ and b/ prefixes
  - With/without app/ prefix
  - Partial path matching as fallback
- Generate comprehensive path variations for robust matching
- Support nested directory structures and partial matches

### AI Prompt Engineering
- When generating prompts for code review:
  - Always include explicit examples of correct/incorrect snippet formats
  - Emphasize that snippets must be copied EXACTLY from the diff
  - Include the full diff content without truncation
  - Provide clear instructions about preserving diff symbols
  - Use structured JSON output format
  - Include comprehensive project context and coding standards
  - Use quality-first configuration (2M+ tokens, 15min timeout)

### Error Handling Patterns
- For comment posting failures:
  - Continue processing other comments instead of failing entirely
  - Track failed comments and include them in the summary
  - Log detailed error information for debugging
  - Use try-catch blocks around individual comment posts
  - Prioritize line numbers over snippet matching (more reliable)
- For batch processing failures:
  - Isolate errors to individual batches
  - Continue processing remaining batches
  - Provide detailed error reporting in final summary

### Known Limitations
- Bitbucket API may have its own behavior for line positioning
- Complex diff structures can make line number extraction challenging
- Some comments may still appear at incorrect lines despite best efforts
- Need to monitor and adjust based on real-world usage patterns
- Very large individual files may still challenge token limits even with batching

## Batch Processing for Large PRs

### When to Use Batch Processing
- Automatically enabled for PRs with >5 files (configurable threshold)
- Essential for enterprise PRs with 50+ files
- Prevents AI token limit violations and service timeouts
- Maintains review quality by avoiding context truncation

### File Prioritization Strategy
- **High Priority**: Security-sensitive files (auth, payment, crypto, admin, config)
- **Medium Priority**: Business logic and core functionality
- **Low Priority**: Documentation, tests, configuration files
- Process high-priority files in early batches for faster security feedback

### Token Management
- Use 70% of AI provider's token limit as safety margin
- Estimate file tokens using 4 characters per token ratio
- Add 1000 token overhead for context and prompts
- Dynamically adjust batch sizes based on token budget

### Batch Configuration Best Practices
- Default: 3 files per batch for optimal balance
- Enable security file prioritization by default
- Use 1000ms delay between batches to avoid rate limits
- Process batches sequentially for better error isolation
- Monitor batch performance and adjust thresholds as needed

### Error Recovery
- Individual batch failures don't stop the entire review
- Failed batches are reported in the final summary
- Retry logic can be added for transient failures
- Graceful degradation ensures partial results are still valuable

## Project Structure

### Import Patterns
- Use dynamic imports for MCP modules: `const dynamicImport = eval('(specifier) => import(specifier)');`
- This bypasses TypeScript compilation issues with ESM modules

### File Organization
- Core logic in `src/core/`
- Feature modules in `src/features/`
- Utility functions in `src/utils/`
- Type definitions in `src/types/`

### Caching Strategy
- Cache keys format: `{operation}:{workspace}:{repository}:{identifier}:{optional_params}`
- Different TTLs:
  - Branch info: 1 hour
  - PR details: 30 minutes
  - PR diff: 30 minutes
  - File content: 2 hours
  - Directory listings: 1 hour

## Testing Patterns

### Testing PR Comment Posting
- Test with PRs containing:
  - Added lines (+ prefix)
  - Removed lines (- prefix)
  - Context lines (space prefix)
  - Mixed changes in same file
  - Large diffs (50+ files)
  - Complex nested file paths

### Common Test Cases
- PR #24488 (JP-6994-ProviderEndpointCRUD) - Good for testing ReScript code
- Test with both small and large PRs
- Verify cache hit ratios
- Check error handling with invalid snippets
- Monitor actual line number placement in Bitbucket UI

## Performance Optimization

### Unified Context Pattern
- Always gather all context in one phase
- Share context between features
- Use smart diff strategies based on PR size:
  - < 20 files: whole diff
  - >= 20 files: file-by-file
  - Large files: consider chunking

### API Call Reduction
- Target: 90% reduction vs separate scripts
- Use caching aggressively
- Batch operations where possible
- Reuse context across features

## Security Considerations

### Credential Handling
- Never log tokens or credentials
- Use environment variables only
- Validate credentials on startup
- Sanitize error messages

### Input Validation
- Always validate PR identifiers
- Sanitize file paths
- Check response structures
- Handle malformed API responses

## Known Issues and Workarounds

### MCP Snippet Matching
- Issue: MCP requires exact string matches
- Workaround: Validate snippets before posting, use 'best' match strategy

### Line Number Accuracy
- Issue: Comments may appear at incorrect lines
- Root cause: Complex diff parsing and line number calculation
- Workaround: Prioritize line numbers when available, fall back to snippets

### Large PR Performance
- Issue: Slowdown with 100+ files
- Workaround: Use file-by-file diff strategy

### Binary Files
- Issue: Cannot analyze binary files
- Workaround: Skip them in analysis

### Cache JSON Parsing Errors with Plain Text Files (RESOLVED)
- Issue: Cache system trying to parse plain text files (like .clinerules) as JSON
- Root cause: BitbucketProvider.getFileContent() assumed all MCP responses were JSON
- Error: "Unexpected token 'F', \"File '.cli\"... is not valid JSON"
- Resolution: Added try-catch in getFileContent() with fallback to plain text
- Pattern: Always handle mixed content types in file retrieval operations
- Files affected: src/core/providers/BitbucketProvider.ts
- Date resolved: 2025-09-24

### Similarity Threshold Standardization (RESOLVED)
- Issue: Inconsistent similarity scales (0-1 vs 0-100) across different features
- Root cause: multiInstance.deduplication.similarityThreshold used 0.4 (0-1 scale) while semanticDeduplication.similarityThreshold used 70 (0-100 scale)
- Resolution: Standardized all similarity thresholds to 0-100 percentage scale
- Changes made:
  - Updated yama.config.example.yaml: multiInstance.deduplication.similarityThreshold from 0.4 to 40
  - Updated yama.config.yaml: multiInstance.deduplication.similarityThreshold from 0.4 to 40
  - Updated type definitions to clarify "0-100 percentage scale" in comments
  - All validation logic already expected 0-100 scale
  - All tests already used 0-100 scale
- Pattern: Always use 0-100 percentage scale for similarity thresholds to avoid misconfiguration

### Parallel Batch Processing Race Conditions (RESOLVED)
- Issue: All batches except first failing with "Insufficient token budget" errors and duplicate processing
- Root cause: Multiple interconnected issues in parallel processing system
  1. Token budget race condition in allocateForBatch() method
  2. Duplicate processing attempts due to lack of proper state management
  3. Floating-point precision errors in token distribution (52428.8 → 52429 allocation)
  4. Pre-allocation logic flaw allowing duplicate allocations
- Resolution: Comprehensive fix with enhanced TokenBudgetManager and proper state management
- Changes made:
  - Enhanced TokenBudgetManager with pre-allocation mode tracking
  - Added batch state management (pending → processing → completed/failed)
  - Fixed floating-point precision by flooring budgets in constructor and calculations
  - Implemented proper duplicate allocation prevention
  - Added comprehensive error handling and recovery
  - Enhanced token distribution with integer arithmetic and remainder handling
  - Added 15 new tests covering all edge cases and race conditions
- Pattern: Always use integer arithmetic for token budgets and implement proper state management for concurrent operations

## Development Workflow

### Building
```bash
npm run build  # Compile TypeScript
npm test       # Run tests
npm run lint   # Check code quality
```

### Running
```bash
# Via CLI
node dist/index.js guardian --branch <branch-name>

# With all features
node dist/index.js guardian --branch <branch-name> --review --enhance
```

### Debugging
- Set LOG_LEVEL=debug for verbose output
- Check cache stats with --verbose flag
- Monitor API call counts
- Review failed comment details in summary
- Check actual line placement in Bitbucket UI

## Git Workflow

### Branch Naming
- Feature: `feature/<ticket>-<description>`
- Fix: `fix/<ticket>-<description>`
- Release: `release/<version>`

### Commit Messages
- Format: `<type>: <subject>`
- Types: feat, fix, docs, style, refactor, test, chore
- Include ticket number when applicable

## AI Provider Management

### Token Limit Patterns
- Provider-specific token limits must be enforced at runtime
- Vertex AI: Conservative limit of 65,536 tokens (API limit is 65,537 exclusive)
- OpenAI: Up to 120,000 tokens for comprehensive analysis
- Anthropic: Up to 190,000 tokens for large PRs
- Auto mode: Conservative 60,000 tokens for reliability
- Always validate configured limits against provider capabilities

### Configuration Validation Patterns
- Runtime validation is more robust than static validation
- Provider-specific constraints may not be known until runtime
- Implement dynamic configuration adjustment with logging
- Use conservative fallbacks when configured limits exceed provider capacity

## Memory Bank Integration

### Configuration Patterns
- Memory bank path should be configurable with fallback discovery
- Default path: "memory-bank"
- Fallback paths: ["docs/memory-bank", ".memory-bank"]
- Enable/disable memory bank features via configuration
- Integration with Cline's memory bank system for context preservation

## Summary Comment Configuration (NEW)
- Added `postSummaryComment` boolean option to CodeReviewConfig
- Controls whether the comprehensive summary comment is posted at the end of reviews
- Default: true (maintains backward compatibility)
- Set to false to disable summary comments and only post inline comments
- Useful for teams that prefer minimal comment footprint or have their own summary mechanisms
- Configuration location: `features.codeReview.postSummaryComment`

### Context Management Patterns
- YAML-based memory bank files for better merge handling
- Automatic memory bank discovery and loading
- Persistent project context and pattern learning across sessions
- Document patterns discovered during development

## Array Mutation Prevention Patterns

### Map-Based Deduplication (RESOLVED)
- Issue: Array mutation during iteration in `removeSameLineDuplicates` method
- Root cause: Direct array index manipulation (`unique[existingIndex] = violation`) was fragile and error-prone
- Resolution: Implemented Map-based approach using unique keys (`${file}_${originalIndex}`)
- Benefits:
  - No array mutation during processing
  - Deterministic unique keys prevent reference-based errors
  - O(1) Map operations vs O(n) array operations
  - Safer updates through atomic Map operations
  - Clearer intent and more maintainable code
- Pattern: Always use Map for managing unique collections during complex processing
- Tests added: Multiple severity upgrades, complex duplicate resolution scenarios

### Safe Collection Management Patterns
- Use Map with deterministic keys instead of array mutation
- Avoid `findIndex` and direct array assignment during iteration
- Prefer atomic operations (Map.set/delete) over array manipulation
- Generate unique keys using stable identifiers (file + originalIndex)
- Process collections immutably when possible

## Future Considerations

### Planned Enhancements
- Improve line number extraction accuracy
- Add metrics for comment placement success
- GitHub and GitLab provider support
- Webhook-based triggers
- Persistent caching with Redis
- Custom rule configuration
- Performance monitoring dashboard
- Enhanced memory bank integration with automatic pattern learning
- Telemetry for configuration validation and adjustment events

### Technical Debt
- Line number extraction needs more robust implementation
- Consider splitting large files
- Add more comprehensive error types
- Implement retry logic for transient failures
- Add telemetry for usage analytics
- Monitor token limit effectiveness in production
- Add metrics for configuration validation and adjustment events
